{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486b5c63",
   "metadata": {},
   "source": [
    "# Graph Convolutional Network with Numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4afd794",
   "metadata": {},
   "source": [
    "### Introduction and libraries\n",
    "\n",
    "We are going to build a Graph Convolutional Network (GCN) similar to the one described by Kipf and Welling (2016) and train it in a node classification task using the Cora dataset (Sen et al., 2008). We are going to implement this from scratch, without using deep learning frameworks such as Tensorflow or Pytorch.\n",
    "\n",
    "Therefore, the only libraries we need are numpy (for mathematical operations) and spektral (for downloading the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540b2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT TO INSTALL \n",
    "\n",
    "#!pip install spektral\n",
    "\n",
    "import numpy as np\n",
    "import spektral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970d36c",
   "metadata": {},
   "source": [
    "The first thing to do is to download the dataset and extract the variables we need for building and training the model. These are:\n",
    "\n",
    "-`train_mask`, `val_mask` and `test_mask`: binary vectors indicating which nodes are to be included in the training, validation and test set respectively.\n",
    "\n",
    "-`features`: a matrix which includes features vector corresponding to each node. Each node represents a scientific article, and each feature indicates whether a certain key word is present in that specific paper. \n",
    "\n",
    "-`adj`: the adjacency matrix specifying the nodes' connections. These indicate the citation network, i.e. which paper was cited in which other paper.\n",
    "\n",
    "-`labels`: The labels of each node, corresponding to the topic of the scientific paper. Labels are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15cd4d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "cora_dataset = spektral.datasets.citation.Citation(name='cora')\n",
    "\n",
    "train_mask = cora_dataset.mask_tr\n",
    "val_mask = cora_dataset.mask_va\n",
    "test_mask = cora_dataset.mask_te\n",
    "\n",
    "graph = cora_dataset.graphs[0]\n",
    "features = graph.x\n",
    "adj = graph.a\n",
    "labels = graph.y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbcd48",
   "metadata": {},
   "source": [
    "### Data exploration and preprocessing\n",
    "\n",
    "Now we can do a little data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c111d939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: 140\n",
      "Size of the validation set: 500\n",
      "Size of the test set: 1000\n",
      "Size of the feature matrix: (2708, 1433)\n",
      "Number of labels: 7\n",
      "Feature value range: 0.0 - 1.0\n",
      "Adjacency matrix type: <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print('Size of the training set:', np.sum(train_mask))\n",
    "print('Size of the validation set:', np.sum(val_mask))\n",
    "print('Size of the test set:', np.sum(test_mask))\n",
    "\n",
    "print('Size of the feature matrix:', features.shape)\n",
    "print('Number of labels:', labels.shape[1])\n",
    "\n",
    "print('Feature value range:', np.min(features), '-', np.max(features))\n",
    "\n",
    "print('Adjacency matrix type:', type(adj))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78efc00e",
   "metadata": {},
   "source": [
    "It looks like we have a very small dataset for training (140 nodes). There are 7 possible labels, and the feature values range from 0 to 1, suggesting they are categorical. Let's quickly verify this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d64d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature unique values: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Feature unique values:', np.unique(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610cfcf",
   "metadata": {},
   "source": [
    "It seems to indeed be the case.\n",
    "\n",
    "We also have a feature space of size 1433, and 2708 total nodes. This means our adjacency matrix should be of size 2703x2703. However, here this is encoded as a sparse matrix, and a dense one would suit our purpose better, as it can be easily used for matrix operations when implementing the model. Let's convert it to a dense representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4427022",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40307acc",
   "metadata": {},
   "source": [
    "### Loss and activation functions \n",
    "\n",
    "Let's now define a few useful functions.\n",
    "\n",
    "-`softmax`: the softmax activation function, which we are going to need to apply to the last layer of our network. It will allow us to have probabilities that sum up to 1 as an output.\n",
    "\n",
    "-`relu`: the relu activation function, to be used in hidden layers. It is a very common way to add some non-linearity to a model.\n",
    "\n",
    "-`masked_loss`: a categorical crossentropy loss with a mask applied to it, so that we can make sure to train and evaluate the model on the right data. The mask needs to be proprocessed to enter the function, but we'll see that later.\n",
    "\n",
    "-`masked_accuracy`: a function to calculate accuracy with a mask applied top it, so that we evaluate our model with the right dataset. As above, the mask needs preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0492bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    num = np.exp(logits)\n",
    "    den = np.sum(np.exp(logits), axis = -1)\n",
    "    den = np.reshape(den, (den.shape[0], 1))\n",
    "    return num/den\n",
    "\n",
    "def relu(logits):\n",
    "    ispos = logits>0\n",
    "    ispos = ispos.astype('float32')\n",
    "    return np.multiply(logits, ispos)\n",
    "\n",
    "def masked_loss(logits, labels, mask):\n",
    "    loss = -np.multiply(labels, np.log(softmax(logits)))\n",
    "    loss = np.multiply(loss, mask)\n",
    "    loss = np.sum(loss, axis=1)\n",
    "    return np.mean(loss)\n",
    "\n",
    "def masked_accuracy(logits, labels, mask):    \n",
    "    pred = logits.argmax(1)\n",
    "    real = np.argmax(labels, axis=-1)\n",
    "    real = np.reshape(real, pred.shape)\n",
    "    is_correct = np.equal(pred, real)\n",
    "    is_correct = np.multiply(is_correct, mask)\n",
    "    return np.mean(is_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5564a5",
   "metadata": {},
   "source": [
    "### Adam optimizer\n",
    "\n",
    "As we are not allowed to use Tensorflow or some other deep learning framework, we have to implement our optimizer from scratch. As in the original paper (Kipf & Welling, 2016), we use an Adam optimizer (Kingma & Ba, 2014), a very common choice.\n",
    "\n",
    "Here we use a standard hyperparameter setting (default in Tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241fdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    def __init__(self, n_layers=2, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.m_dW = []\n",
    "        self.v_dW = []\n",
    "        self.m_db = []\n",
    "        self.v_db = []\n",
    "        for l in range(n_layers):\n",
    "            self.m_dW.append(0)\n",
    "            self.v_dW.append(0)\n",
    "            self.m_db.append(0)\n",
    "            self.v_db.append(0)\n",
    "            \n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "    def update(self, t, W, b, dW, db):\n",
    "        for l in range(len(W)):\n",
    "            ## momentum beta 1\n",
    "            # *** weights *** #\n",
    "            self.m_dW[l] = self.beta1*self.m_dW[l] + (1-self.beta1)*dW[l]\n",
    "            # *** biases *** #\n",
    "            self.m_db[l] = self.beta1*self.m_db[l] + (1-self.beta1)*db[l]\n",
    "\n",
    "            ## rms beta 2\n",
    "            # *** weights *** #\n",
    "            self.v_dW[l] = self.beta2*self.v_dW[l] + (1-self.beta2)*np.power(dW[l], 2)\n",
    "            # *** biases *** #\n",
    "            self.v_db[l] = self.beta2*self.v_db[l] + (1-self.beta2)*np.power(db[l], 2)\n",
    "\n",
    "            ## bias correction\n",
    "            m_dW_corr = self.m_dW[l]/(1-np.power(self.beta1, t))\n",
    "            m_db_corr = self.m_db[l]/(1-np.power(self.beta1, t))\n",
    "            v_dW_corr = self.v_dW[l]/(1-np.power(self.beta2, t))\n",
    "            v_db_corr = self.v_db[l]/(1-np.power(self.beta2, t))\n",
    "\n",
    "            ## update weights and biases\n",
    "            W[l] = W[l] - self.eta*(m_dW_corr/(np.sqrt(v_dW_corr)+self.epsilon))\n",
    "            b[l] = b[l] - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n",
    "        return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98052e02",
   "metadata": {},
   "source": [
    "### Some building blocks\n",
    "We can now start building the model itself. First of all we are going to define two functions:\n",
    "\n",
    "-`gnn_layer`: a function defining a forward pass in a single Graph Neural Network (GNN) layer. It uses matrix operations for to perform message passing, which makes the process more efficient (the alternative being nested for loops, which are both slower and harder to read). It takes as input the nodes `features`, an adjacency matrix `adj`, weigth matrix `W` and bias vector `b`, as well as some `activation` function. It returns all the processing stages of the forward pass, which we are going to need for the backpropagation. This is a very general-purpose function, and, as we'll see later, can be used for models other than GCNs.\n",
    "\n",
    "-`gnn_forward`: a function performing the whole forward pass for an arbitrary number of layers. The inputs `features` and `adj` are the same as above, however, `W` and `b` are now lists, with as many entries as there are layers in the network. `n_units` is also a list, specifying how many units there are in each layer (including the output layer). For all layers but the output one the function will apply the `relu` activation and an additional dropout layer with a rate specified by the parameter `dropout`. The final layer has no dropout and has a `softmax` activation function applied to it to output label probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98df7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_layer(features, adj, W, b, activation):\n",
    "    h = np.dot(features, W) + b \n",
    "    z = np.dot(adj, h) \n",
    "    a = activation(z)\n",
    "    return h, z, a\n",
    "\n",
    "def gnn_forward(features, n_units, adj, W, b, dropout):\n",
    "    H = []\n",
    "    Z = []\n",
    "    A = []\n",
    "    \n",
    "    for l, n in enumerate(n_units):\n",
    "\n",
    "        if l == 0: #First layer\n",
    "            h, z, a = gnn_layer(features, adj, W[l], b[l], relu)           \n",
    "        elif l < len(n_units)-1: #Middle layers\n",
    "            h, z, a = gnn_layer(a, adj, W[l], b[l], relu)\n",
    "        else: #Output layer\n",
    "            h, z, a = gnn_layer(a, adj, W[l], b[l], softmax)\n",
    "        \n",
    "        #Dropout\n",
    "        if dropout > 0 and l < len(n_units)-1:\n",
    "            p = np.random.uniform(size=(1, h.shape[1]))\n",
    "            keep = p > dropout\n",
    "            h = np.multiply(h, keep)\n",
    "            z = np.multiply(z, keep)\n",
    "            a = np.multiply(a, keep)\n",
    "\n",
    "        H.append(h)\n",
    "        Z.append(z)\n",
    "        A.append(a)\n",
    "    \n",
    "    return H, Z, A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc041d",
   "metadata": {},
   "source": [
    "### The model itself\n",
    "We are building a `GNN` class, so that we can later build different instances of it and play around with hyperparameters. \n",
    "\n",
    "The method `__init__`, called to initialise an instance of the class, takes as input the number of node features `n_features`, a list containing the number of units in each layer `n_units` and a `dropout` layer to be applied only during training to prevent overfitting. All the weights and biases are initialised by sampling from a normal distribution with mean 0 and standard deviation 0.01, and stored as lists (`W` and `b` respectively).\n",
    "\n",
    "The method `fit` trains the model, taking as input the nodes' `features` and `labels`, the number of epochs `n_epochs` we want to train our model for, an `optimizer` for updating the weights and the training and validation masks (`train_mask` and `val_mask`) to train and evaluate the model on the nodes corresponding to the trainin and validation datasets, respectively.\n",
    "The masks are divided by their mean value, making them easier to use for calculating the loss and accuracy (see above). The method loops over epochs. For each iteration, it performs a forward pass to calculate outputs and a backwards pass (back propagation) to calculate the gradients of the loss with respect to the trainable parameters contained in the lists `W` and `b` (i.e. weights and biases). It then applies the Adam optimizer (defined above) to update such parameters. Finally, it evaluates the loss and accuracy for both the training and validation nodes, storing the weigths and biases in the lists `W_best` and `b_best` only when the validation loss is lower than its previous minimum value. \n",
    "\n",
    "Finally, the method `evaluate` uses `W_best` and `b_best` to evaluate the model performance on a certain set of nodes determined by the input parameter `mask`, returning the value of the `loss` and the `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "849c61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN():\n",
    "    def __init__(self, n_features, adj, n_units, dropout):\n",
    "        \n",
    "        self.adj = adj\n",
    "        self.n_units = n_units\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #Initialise all weights as an empty list\n",
    "        W = []\n",
    "        b = []\n",
    "        for l, n in enumerate(n_units):\n",
    "            \n",
    "            #Get dimensions for weight matrix \n",
    "            if l == 0:\n",
    "                dims = (n_features, n) #Original features\n",
    "            else:\n",
    "                dims = (n_units[l-1], n)\n",
    "            \n",
    "            #Initialise weight matrix (random normal initalisation)\n",
    "            W.append(np.random.normal(0, 0.01, dims))\n",
    "            b.append(np.random.normal(0, 0.01, (1, n)))\n",
    "         \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.best_W = W\n",
    "        self.best_b = b\n",
    "        \n",
    "    def fit(self, features, labels, n_epochs, optimizer, \n",
    "           train_mask, val_mask):\n",
    "        \n",
    "        #Preprocess the masks for easier use \n",
    "        train_mask = train_mask.astype('float32')\n",
    "        train_mask /= np.mean(train_mask)\n",
    "        train_mask = train_mask.reshape(train_mask.shape[0], 1)\n",
    "        \n",
    "        val_mask = val_mask.astype('float32')\n",
    "        val_mask /= np.mean(val_mask)\n",
    "        val_mask = val_mask.reshape(val_mask.shape[0], 1)\n",
    "        \n",
    "        #Initialise best val loss to large number\n",
    "        best_val_loss = 1e10 \n",
    "        \n",
    "        for e in range(n_epochs):\n",
    "            \n",
    "            #Forward prop\n",
    "            H, Z, A = gnn_forward(features, self.n_units, self.adj, \n",
    "                                  self.W, self.b, dropout=self.dropout)\n",
    "\n",
    "            #Backprop\n",
    "            dW = []\n",
    "            db = []\n",
    "            dH = []\n",
    "            counter = -1\n",
    "\n",
    "            for l in range(len(self.n_units)-1, -1, -1):\n",
    "\n",
    "                counter +=1\n",
    "                if l == len(self.n_units)-1:\n",
    "                    dz = np.multiply((A[l] - labels), train_mask) #Mask to take only the gradient for training examples\n",
    "                    dh = np.dot(self.adj.T, dz) \n",
    "                    dw = np.dot(A[l-1].T, dh)\n",
    "                elif l==0:\n",
    "                    da = np.dot(dH[counter-1], self.W[l+1].T)\n",
    "                    dz = np.multiply(da, Z[l]>0)\n",
    "                    dh = np.dot(self.adj.T, dz)\n",
    "                    dw = np.dot(features.T, dh)\n",
    "                else:\n",
    "                    da = np.dot(dH[counter-1], self.W[l+1].T)\n",
    "                    dz = np.multiply(da, Z[l]>0)\n",
    "                    dh = np.dot(self.adj, dz)\n",
    "                    dw = np.dot(A[l-1].T, dh)\n",
    "                    \n",
    "                dH.append(dh)\n",
    "                dW.append(dw)\n",
    "                db.append(np.mean(dh, axis=0))\n",
    "\n",
    "            dW.reverse()\n",
    "            db.reverse()\n",
    "\n",
    "            self.W, self.b = optimizer.update(e+1, self.W, self.b, \n",
    "                                                        dW, db)\n",
    "            \n",
    "            #Forward pass for calculating losses and accuracies\n",
    "            _, _, A = gnn_forward(features, self.n_units, self.adj, \n",
    "                                  self.W, self.b, dropout=0)\n",
    "            \n",
    "            #Calculate losses and accuracies\n",
    "            train_loss = masked_loss(A[-1], labels, train_mask)\n",
    "            val_loss = masked_loss(A[-1], labels, val_mask)\n",
    "            \n",
    "            train_accuracy = masked_accuracy(A[-1], labels, train_mask)\n",
    "            val_accuracy = masked_accuracy(A[-1], labels, val_mask)\n",
    "            \n",
    "            print('Epoch ', e+1, ': \\n'\n",
    "                 'Training loss: ', train_loss, ' | ', \n",
    "                 'Validation loss: ', val_loss, ' | \\n',\n",
    "                 'Training accuracy: ', train_accuracy, ' | ', \n",
    "                 'Validation accuracy: ', val_accuracy, ' | ')\n",
    "            \n",
    "            #Update best weights only if validation loss has improved\n",
    "            if val_loss < best_val_loss:\n",
    "                self.best_W = self.W\n",
    "                self.best_b = self.b\n",
    "    \n",
    "    def evaluate(self, features, labels, mask):\n",
    "        \n",
    "        mask = mask.astype('float32')\n",
    "        mask /= np.mean(mask)\n",
    "        mask = mask.reshape(mask.shape[0], 1)\n",
    "        \n",
    "        _, _, A = gnn_forward(features, self.n_units, self.adj, \n",
    "                           self.best_W, self.best_b, dropout=0)\n",
    "        \n",
    "        loss = masked_loss(A[-1], labels, mask)\n",
    "        accuracy = masked_accuracy(A[-1], labels, mask)\n",
    "        \n",
    "        return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d7460",
   "metadata": {},
   "source": [
    "### Building a Graph Convolutional Network\n",
    "\n",
    "Before we can create an instance of our model, we have to go through one final step, modifying the adjacency matrix as in the paper we are using as a guide (Kipf & Welling, 2016).\n",
    "\n",
    "We can then finally build A GCN. We are using a 64-dimensional node representation. We are also using a dropout rate of 0.5, as in the original paper (Kipf & Welling, 2016). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6001bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise the adjecency matrix as in Kipf & Welling (2017)\n",
    "adj += np.eye(adj.shape[0])\n",
    "D = np.sum(adj, axis=-1)\n",
    "I = np.eye(adj.shape[0])\n",
    "D_norm = np.multiply(I, (1/np.sqrt(D)))\n",
    "adj_norm = np.dot(D_norm, np.dot(adj, D_norm))\n",
    "\n",
    "GCN = GNN(features.shape[1], adj_norm, [64, 7], 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa2393",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "We can now create an instance of our optimiser (with default settings) and `fit` our model to the training set. We are training for 100 epochs, but the model is going to take much less to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a389f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 : \n",
      "Training loss:  1.9445570730261017  |  Validation loss:  1.9448956901334866  | \n",
      " Training accuracy:  0.14285715  |  Validation accuracy:  0.162  | \n",
      "Epoch  2 : \n",
      "Training loss:  1.941682052896121  |  Validation loss:  1.9437417170029123  | \n",
      " Training accuracy:  0.42142856  |  Validation accuracy:  0.26599997  | \n",
      "Epoch  3 : \n",
      "Training loss:  1.9364550846000994  |  Validation loss:  1.941491951596453  | \n",
      " Training accuracy:  0.5  |  Validation accuracy:  0.27199998  | \n",
      "Epoch  4 : \n",
      "Training loss:  1.928627213174946  |  Validation loss:  1.938123359756131  | \n",
      " Training accuracy:  0.60714287  |  Validation accuracy:  0.274  | \n",
      "Epoch  5 : \n",
      "Training loss:  1.9173838089983026  |  Validation loss:  1.9332656569670923  | \n",
      " Training accuracy:  0.73571426  |  Validation accuracy:  0.32999998  | \n",
      "Epoch  6 : \n",
      "Training loss:  1.9012335643424556  |  Validation loss:  1.9261958728313  | \n",
      " Training accuracy:  0.7928571  |  Validation accuracy:  0.39599997  | \n",
      "Epoch  7 : \n",
      "Training loss:  1.878942880283317  |  Validation loss:  1.9165769791459264  | \n",
      " Training accuracy:  0.8499999  |  Validation accuracy:  0.45599997  | \n",
      "Epoch  8 : \n",
      "Training loss:  1.8497314818628117  |  Validation loss:  1.9040423539950715  | \n",
      " Training accuracy:  0.88571423  |  Validation accuracy:  0.52  | \n",
      "Epoch  9 : \n",
      "Training loss:  1.8124154029760078  |  Validation loss:  1.887888064190414  | \n",
      " Training accuracy:  0.92142856  |  Validation accuracy:  0.584  | \n",
      "Epoch  10 : \n",
      "Training loss:  1.7654420996145705  |  Validation loss:  1.8676400448589736  | \n",
      " Training accuracy:  0.94285715  |  Validation accuracy:  0.63199997  | \n",
      "Epoch  11 : \n",
      "Training loss:  1.7083376701649324  |  Validation loss:  1.842948563370885  | \n",
      " Training accuracy:  0.95000005  |  Validation accuracy:  0.67199993  | \n",
      "Epoch  12 : \n",
      "Training loss:  1.6430941569035853  |  Validation loss:  1.8136949470230121  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.686  | \n",
      "Epoch  13 : \n",
      "Training loss:  1.575621867504994  |  Validation loss:  1.7805624817813581  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.70399994  | \n",
      "Epoch  14 : \n",
      "Training loss:  1.5098632598345783  |  Validation loss:  1.7446942613032281  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.7139999  | \n",
      "Epoch  15 : \n",
      "Training loss:  1.4482624882840494  |  Validation loss:  1.7078640929166355  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.72199994  | \n",
      "Epoch  16 : \n",
      "Training loss:  1.3937863198245986  |  Validation loss:  1.6725208869775243  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.728  | \n",
      "Epoch  17 : \n",
      "Training loss:  1.3466214635662057  |  Validation loss:  1.637549067968853  | \n",
      " Training accuracy:  0.97857153  |  Validation accuracy:  0.738  | \n",
      "Epoch  18 : \n",
      "Training loss:  1.30826703378358  |  Validation loss:  1.605077133293217  | \n",
      " Training accuracy:  0.97857153  |  Validation accuracy:  0.74399996  | \n",
      "Epoch  19 : \n",
      "Training loss:  1.2773188922971361  |  Validation loss:  1.5742557780175845  | \n",
      " Training accuracy:  0.97857153  |  Validation accuracy:  0.752  | \n",
      "Epoch  20 : \n",
      "Training loss:  1.253570355222318  |  Validation loss:  1.547573412396594  | \n",
      " Training accuracy:  0.97857153  |  Validation accuracy:  0.756  | \n",
      "Epoch  21 : \n",
      "Training loss:  1.2337787811849745  |  Validation loss:  1.520651848717364  | \n",
      " Training accuracy:  0.97857153  |  Validation accuracy:  0.766  | \n",
      "Epoch  22 : \n",
      "Training loss:  1.218993535764259  |  Validation loss:  1.4999984597186182  | \n",
      " Training accuracy:  0.97857153  |  Validation accuracy:  0.766  | \n",
      "Epoch  23 : \n",
      "Training loss:  1.2066020743152672  |  Validation loss:  1.4786261564125986  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.77199996  | \n",
      "Epoch  24 : \n",
      "Training loss:  1.1973215905382402  |  Validation loss:  1.4644930495623516  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.7719999  | \n",
      "Epoch  25 : \n",
      "Training loss:  1.1904479805973964  |  Validation loss:  1.4516803117667871  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.7819999  | \n",
      "Epoch  26 : \n",
      "Training loss:  1.1856057138009928  |  Validation loss:  1.4399025596165278  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.7879999  | \n",
      "Epoch  27 : \n",
      "Training loss:  1.1819110987607016  |  Validation loss:  1.4307253778742042  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.786  | \n",
      "Epoch  28 : \n",
      "Training loss:  1.178984242663374  |  Validation loss:  1.4221365902427814  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.79  | \n",
      "Epoch  29 : \n",
      "Training loss:  1.1782353726818409  |  Validation loss:  1.41891016438664  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.78  | \n",
      "Epoch  30 : \n",
      "Training loss:  1.177962481049998  |  Validation loss:  1.4197746113974252  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.774  | \n",
      "Epoch  31 : \n",
      "Training loss:  1.178300700382042  |  Validation loss:  1.4213711283535315  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.77  | \n",
      "Epoch  32 : \n",
      "Training loss:  1.1774016087503034  |  Validation loss:  1.4212498227918864  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.77  | \n",
      "Epoch  33 : \n",
      "Training loss:  1.1760812975461303  |  Validation loss:  1.420247985600443  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.76399994  | \n",
      "Epoch  34 : \n",
      "Training loss:  1.1748669299524812  |  Validation loss:  1.420204467672928  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.76399994  | \n",
      "Epoch  35 : \n",
      "Training loss:  1.1733540126080266  |  Validation loss:  1.420781188235843  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  36 : \n",
      "Training loss:  1.1714228870178716  |  Validation loss:  1.4204137332431963  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.762  | \n",
      "Epoch  37 : \n",
      "Training loss:  1.1689457626531885  |  Validation loss:  1.418715639791818  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  38 : \n",
      "Training loss:  1.167342450754284  |  Validation loss:  1.4164489053446037  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.756  | \n",
      "Epoch  39 : \n",
      "Training loss:  1.1669207176484104  |  Validation loss:  1.4156821775735378  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.754  | \n",
      "Epoch  40 : \n",
      "Training loss:  1.1666261910126667  |  Validation loss:  1.4156169522584308  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.754  | \n",
      "Epoch  41 : \n",
      "Training loss:  1.1663711615655235  |  Validation loss:  1.4152137910228226  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  42 : \n",
      "Training loss:  1.1662310354704957  |  Validation loss:  1.4151872481002796  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.762  | \n",
      "Epoch  43 : \n",
      "Training loss:  1.1660953910719045  |  Validation loss:  1.414855383237446  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  44 : \n",
      "Training loss:  1.1659964562256688  |  Validation loss:  1.414700293000505  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75999993  | \n",
      "Epoch  45 : \n",
      "Training loss:  1.1659122040493668  |  Validation loss:  1.4149060960267426  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  46 : \n",
      "Training loss:  1.1658079420080816  |  Validation loss:  1.4143950467912365  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  47 : \n",
      "Training loss:  1.165844773240377  |  Validation loss:  1.4132053783587837  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76400006  | \n",
      "Epoch  48 : \n",
      "Training loss:  1.1659356019029785  |  Validation loss:  1.4118223987919074  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.762  | \n",
      "Epoch  49 : \n",
      "Training loss:  1.1661627201029443  |  Validation loss:  1.4108476965507926  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76199996  | \n",
      "Epoch  50 : \n",
      "Training loss:  1.166484267991609  |  Validation loss:  1.4100933105356304  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76199996  | \n",
      "Epoch  51 : \n",
      "Training loss:  1.1667208025582496  |  Validation loss:  1.4097676742887417  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.762  | \n",
      "Epoch  52 : \n",
      "Training loss:  1.166630864151306  |  Validation loss:  1.408980336489064  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  53 : \n",
      "Training loss:  1.166416196500459  |  Validation loss:  1.4085389955266232  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.762  | \n",
      "Epoch  54 : \n",
      "Training loss:  1.166160437439703  |  Validation loss:  1.406733483535344  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76399994  | \n",
      "Epoch  55 : \n",
      "Training loss:  1.1660308863135218  |  Validation loss:  1.4056971340406736  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76799995  | \n",
      "Epoch  56 : \n",
      "Training loss:  1.1658317862782281  |  Validation loss:  1.404499151694695  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76799995  | \n",
      "Epoch  57 : \n",
      "Training loss:  1.1656512939838144  |  Validation loss:  1.403038718270082  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77  | \n",
      "Epoch  58 : \n",
      "Training loss:  1.165561109253002  |  Validation loss:  1.4026575819365428  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77  | \n",
      "Epoch  59 : \n",
      "Training loss:  1.1655280606942764  |  Validation loss:  1.4020974956281465  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.766  | \n",
      "Epoch  60 : \n",
      "Training loss:  1.1655307968702915  |  Validation loss:  1.401749069059727  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.766  | \n",
      "Epoch  61 : \n",
      "Training loss:  1.1655617659855506  |  Validation loss:  1.401660926212126  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  62 : \n",
      "Training loss:  1.1655974668086206  |  Validation loss:  1.4021262813350006  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  63 : \n",
      "Training loss:  1.1656444823615155  |  Validation loss:  1.4024200567519258  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75600004  | \n",
      "Epoch  64 : \n",
      "Training loss:  1.165693686037193  |  Validation loss:  1.4025600636491198  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  65 : \n",
      "Training loss:  1.1657490861764657  |  Validation loss:  1.4027790892630294  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76199996  | \n",
      "Epoch  66 : \n",
      "Training loss:  1.165807792876637  |  Validation loss:  1.4030301839180332  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75999993  | \n",
      "Epoch  67 : \n",
      "Training loss:  1.1658617853682152  |  Validation loss:  1.4033098086033482  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75999993  | \n",
      "Epoch  68 : \n",
      "Training loss:  1.1657988546886004  |  Validation loss:  1.4042254800056504  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  69 : \n",
      "Training loss:  1.1657344973426134  |  Validation loss:  1.4045847102818019  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  70 : \n",
      "Training loss:  1.1657437779707847  |  Validation loss:  1.4045872835153375  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  71 : \n",
      "Training loss:  1.1657544546787564  |  Validation loss:  1.404656706431827  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  72 : \n",
      "Training loss:  1.165672595654319  |  Validation loss:  1.4056119146371655  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  73 : \n",
      "Training loss:  1.1656065436473844  |  Validation loss:  1.4065529289114855  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.762  | \n",
      "Epoch  74 : \n",
      "Training loss:  1.1655431946942154  |  Validation loss:  1.4077032693778049  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75999993  | \n",
      "Epoch  75 : \n",
      "Training loss:  1.1654954231955772  |  Validation loss:  1.4092942778966058  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75399995  | \n",
      "Epoch  76 : \n",
      "Training loss:  1.1654724032864499  |  Validation loss:  1.411539683300189  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.752  | \n",
      "Epoch  77 : \n",
      "Training loss:  1.165466158639911  |  Validation loss:  1.4129750562497045  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.752  | \n",
      "Epoch  78 : \n",
      "Training loss:  1.1654625410966928  |  Validation loss:  1.4135424358714983  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  79 : \n",
      "Training loss:  1.165461175468985  |  Validation loss:  1.414069701849461  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  80 : \n",
      "Training loss:  1.1654550732808238  |  Validation loss:  1.4138710922114721  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  81 : \n",
      "Training loss:  1.1654495598657963  |  Validation loss:  1.4136900397297179  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.746  | \n",
      "Epoch  82 : \n",
      "Training loss:  1.165447909109093  |  Validation loss:  1.413818959176486  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74399996  | \n",
      "Epoch  83 : \n",
      "Training loss:  1.1654463160575306  |  Validation loss:  1.4135556035815293  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.746  | \n",
      "Epoch  84 : \n",
      "Training loss:  1.165446776110411  |  Validation loss:  1.4133458329642883  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  85 : \n",
      "Training loss:  1.1654475520358476  |  Validation loss:  1.4131639491634693  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  86 : \n",
      "Training loss:  1.165449125276302  |  Validation loss:  1.4126732383375808  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  87 : \n",
      "Training loss:  1.165449592906204  |  Validation loss:  1.412072805581004  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  88 : \n",
      "Training loss:  1.165448964201147  |  Validation loss:  1.411615870945838  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  89 : \n",
      "Training loss:  1.1654475906439965  |  Validation loss:  1.4105929403926232  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  90 : \n",
      "Training loss:  1.165446430355108  |  Validation loss:  1.4097194256688463  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75200003  | \n",
      "Epoch  91 : \n",
      "Training loss:  1.1654459641010095  |  Validation loss:  1.4090179905589293  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75200003  | \n",
      "Epoch  92 : \n",
      "Training loss:  1.1654443481666776  |  Validation loss:  1.4083498294817909  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.754  | \n",
      "Epoch  93 : \n",
      "Training loss:  1.165445099387463  |  Validation loss:  1.4075515663010176  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.754  | \n",
      "Epoch  94 : \n",
      "Training loss:  1.165445667716269  |  Validation loss:  1.4068664834003533  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  95 : \n",
      "Training loss:  1.165445974786024  |  Validation loss:  1.4062704205122452  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.752  | \n",
      "Epoch  96 : \n",
      "Training loss:  1.165446652560448  |  Validation loss:  1.40581064855116  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.752  | \n",
      "Epoch  97 : \n",
      "Training loss:  1.165446618337072  |  Validation loss:  1.4054010428008716  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75399995  | \n",
      "Epoch  98 : \n",
      "Training loss:  1.165447948337103  |  Validation loss:  1.4052701054049819  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75399995  | \n",
      "Epoch  99 : \n",
      "Training loss:  1.1654488278464858  |  Validation loss:  1.4051439478846108  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75399995  | \n",
      "Epoch  100 : \n",
      "Training loss:  1.1654487838774377  |  Validation loss:  1.405015515133568  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75200003  | \n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(n_layers = 2)\n",
    "\n",
    "GCN.fit(features, labels, 100, optimizer, \n",
    "           train_mask, val_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc759a",
   "metadata": {},
   "source": [
    "Now we have the weights and biases from the epochs with the smallest validation loss stored  as `W_best` and `b_best`. We can use these to evaluate our model on the training, validation and test nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3602a121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training loss:  1.1654487838774377  |  Training accuracy:  1.0 \n",
      " Validation loss:  1.405015515133568  |  Validation accuracy:  0.75200003 \n",
      " Test loss:  1.3868402164352012  |  Test accuracy:  0.7809999\n"
     ]
    }
   ],
   "source": [
    "loss_train, accuracy_train = GCN.evaluate(features, labels, train_mask)\n",
    "loss_val, accuracy_val = GCN.evaluate(features, labels, val_mask)\n",
    "loss_test, accuracy_test = GCN.evaluate(features, labels, test_mask)\n",
    "\n",
    "print(' Training loss: ', loss_train, ' | ', 'Training accuracy: ', accuracy_train, '\\n', \n",
    "      'Validation loss: ', loss_val, ' | ', 'Validation accuracy: ', accuracy_val, '\\n',\n",
    "      'Test loss: ', loss_test, ' | ', 'Test accuracy: ', accuracy_test)\n",
    "                  \n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76df572",
   "metadata": {},
   "source": [
    "Our test accuracy should be between 75% and 80% (there are fluctuations due to the random weight initialisation), which is pretty close with that reported in the original paper (Kipf & Welling, 2017) for the Cora dataset (81.5%). Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8cfb3",
   "metadata": {},
   "source": [
    "### Comparison with a Multi Layer Perceptron (MLP)\n",
    "How much did the graph structural information encoded in the adjacency matrix influence our results? We can verifying this by training a Multi Layer Perceptron (MLP) on the nodes' features only, neglecting all the edges. To implement this in practice, we can build another instance of `GNN`, using an identity matrix instead of the adjacency one. This will make every node only connected to itself, eliminating all the structural information encoded in the edges.\n",
    "\n",
    "For the sake of a fair comparison, let's keep all the hyper-parameters the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf613539",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = GNN(features.shape[1], np.eye(adj.shape[0]), [64, 7], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12cf4352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 : \n",
      "Training loss:  1.944341985449011  |  Validation loss:  1.9456767244795672  | \n",
      " Training accuracy:  0.23005907  |  Validation accuracy:  0.23005912  | \n",
      "Epoch  2 : \n",
      "Training loss:  1.940832193728074  |  Validation loss:  1.9447602466898972  | \n",
      " Training accuracy:  0.41728202  |  Validation accuracy:  0.41728222  | \n",
      "Epoch  3 : \n",
      "Training loss:  1.9349723555483016  |  Validation loss:  1.9433287092345144  | \n",
      " Training accuracy:  0.45051703  |  Validation accuracy:  0.4505169  | \n",
      "Epoch  4 : \n",
      "Training loss:  1.9257890948183713  |  Validation loss:  1.9414754122133324  | \n",
      " Training accuracy:  0.47562775  |  Validation accuracy:  0.47562793  | \n",
      "Epoch  5 : \n",
      "Training loss:  1.9122526715481416  |  Validation loss:  1.9386857765417886  | \n",
      " Training accuracy:  0.5110783  |  Validation accuracy:  0.51107824  | \n",
      "Epoch  6 : \n",
      "Training loss:  1.8913800972008359  |  Validation loss:  1.9345680697630423  | \n",
      " Training accuracy:  0.52954215  |  Validation accuracy:  0.52954185  | \n",
      "Epoch  7 : \n",
      "Training loss:  1.862517455399314  |  Validation loss:  1.9291773918178972  | \n",
      " Training accuracy:  0.5332349  |  Validation accuracy:  0.53323483  | \n",
      "Epoch  8 : \n",
      "Training loss:  1.8238133185533167  |  Validation loss:  1.9220560114371021  | \n",
      " Training accuracy:  0.5369276  |  Validation accuracy:  0.5369278  | \n",
      "Epoch  9 : \n",
      "Training loss:  1.7730805923608037  |  Validation loss:  1.913240052453006  | \n",
      " Training accuracy:  0.5413589  |  Validation accuracy:  0.5413593  | \n",
      "Epoch  10 : \n",
      "Training loss:  1.7102169038955208  |  Validation loss:  1.902663913100296  | \n",
      " Training accuracy:  0.5376662  |  Validation accuracy:  0.5376665  | \n",
      "Epoch  11 : \n",
      "Training loss:  1.6367527200025256  |  Validation loss:  1.8902480415513716  | \n",
      " Training accuracy:  0.54098976  |  Validation accuracy:  0.54098994  | \n",
      "Epoch  12 : \n",
      "Training loss:  1.5567034581374004  |  Validation loss:  1.8755621491726362  | \n",
      " Training accuracy:  0.5424669  |  Validation accuracy:  0.5424664  | \n",
      "Epoch  13 : \n",
      "Training loss:  1.4768513228524105  |  Validation loss:  1.8587850943541238  | \n",
      " Training accuracy:  0.5465288  |  Validation accuracy:  0.54652846  | \n",
      "Epoch  14 : \n",
      "Training loss:  1.4066425309046338  |  Validation loss:  1.8404925540340349  | \n",
      " Training accuracy:  0.5457903  |  Validation accuracy:  0.5457901  | \n",
      "Epoch  15 : \n",
      "Training loss:  1.3493676571345319  |  Validation loss:  1.8217280569381633  | \n",
      " Training accuracy:  0.545421  |  Validation accuracy:  0.5454206  | \n",
      "Epoch  16 : \n",
      "Training loss:  1.3061425635797699  |  Validation loss:  1.8024782527912908  | \n",
      " Training accuracy:  0.5439439  |  Validation accuracy:  0.5439434  | \n",
      "Epoch  17 : \n",
      "Training loss:  1.2747741828905592  |  Validation loss:  1.7841590858031808  | \n",
      " Training accuracy:  0.5454209  |  Validation accuracy:  0.54542065  | \n",
      "Epoch  18 : \n",
      "Training loss:  1.2512539045395175  |  Validation loss:  1.7657537896737086  | \n",
      " Training accuracy:  0.548006  |  Validation accuracy:  0.54800564  | \n",
      "Epoch  19 : \n",
      "Training loss:  1.2335604559886706  |  Validation loss:  1.7486820022818832  | \n",
      " Training accuracy:  0.55059093  |  Validation accuracy:  0.550591  | \n",
      "Epoch  20 : \n",
      "Training loss:  1.2204219277453274  |  Validation loss:  1.7319602413739814  | \n",
      " Training accuracy:  0.55613  |  Validation accuracy:  0.5561302  | \n",
      "Epoch  21 : \n",
      "Training loss:  1.2104259275241012  |  Validation loss:  1.7168453439094211  | \n",
      " Training accuracy:  0.5583456  |  Validation accuracy:  0.5583454  | \n",
      "Epoch  22 : \n",
      "Training loss:  1.2028524462520322  |  Validation loss:  1.7031321157982702  | \n",
      " Training accuracy:  0.5605614  |  Validation accuracy:  0.5605612  | \n",
      "Epoch  23 : \n",
      "Training loss:  1.1965617109872282  |  Validation loss:  1.6915720125241542  | \n",
      " Training accuracy:  0.5649926  |  Validation accuracy:  0.5649925  | \n",
      "Epoch  24 : \n",
      "Training loss:  1.1913228722107052  |  Validation loss:  1.6808177597652285  | \n",
      " Training accuracy:  0.5661005  |  Validation accuracy:  0.56610054  | \n",
      "Epoch  25 : \n",
      "Training loss:  1.186930556509132  |  Validation loss:  1.6699570375858068  | \n",
      " Training accuracy:  0.570901  |  Validation accuracy:  0.57090133  | \n",
      "Epoch  26 : \n",
      "Training loss:  1.1831346599312034  |  Validation loss:  1.6603683219350294  | \n",
      " Training accuracy:  0.5749631  |  Validation accuracy:  0.57496285  | \n",
      "Epoch  27 : \n",
      "Training loss:  1.1798561727101597  |  Validation loss:  1.6526535066981896  | \n",
      " Training accuracy:  0.57902515  |  Validation accuracy:  0.57902515  | \n",
      "Epoch  28 : \n",
      "Training loss:  1.1770959368074323  |  Validation loss:  1.6457023828286002  | \n",
      " Training accuracy:  0.58087146  |  Validation accuracy:  0.58087164  | \n",
      "Epoch  29 : \n",
      "Training loss:  1.1748092919742308  |  Validation loss:  1.6392170940884705  | \n",
      " Training accuracy:  0.58271796  |  Validation accuracy:  0.5827182  | \n",
      "Epoch  30 : \n",
      "Training loss:  1.172943846671391  |  Validation loss:  1.632974022024813  | \n",
      " Training accuracy:  0.5853029  |  Validation accuracy:  0.5853031  | \n",
      "Epoch  31 : \n",
      "Training loss:  1.171426822544935  |  Validation loss:  1.6280649451641835  | \n",
      " Training accuracy:  0.58567214  |  Validation accuracy:  0.5856722  | \n",
      "Epoch  32 : \n",
      "Training loss:  1.1701914077439397  |  Validation loss:  1.6230993563104708  | \n",
      " Training accuracy:  0.5886264  |  Validation accuracy:  0.5886257  | \n",
      "Epoch  33 : \n",
      "Training loss:  1.169266038455549  |  Validation loss:  1.6186404368609384  | \n",
      " Training accuracy:  0.58530295  |  Validation accuracy:  0.585303  | \n",
      "Epoch  34 : \n",
      "Training loss:  1.168506258041718  |  Validation loss:  1.6148993283983153  | \n",
      " Training accuracy:  0.5864107  |  Validation accuracy:  0.5864108  | \n",
      "Epoch  35 : \n",
      "Training loss:  1.1679085756531262  |  Validation loss:  1.6117332487962985  | \n",
      " Training accuracy:  0.5875184  |  Validation accuracy:  0.5875185  | \n",
      "Epoch  36 : \n",
      "Training loss:  1.1674261482790376  |  Validation loss:  1.6100288207404498  | \n",
      " Training accuracy:  0.58641064  |  Validation accuracy:  0.5864109  | \n",
      "Epoch  37 : \n",
      "Training loss:  1.167048432824212  |  Validation loss:  1.6082305769435223  | \n",
      " Training accuracy:  0.588257  |  Validation accuracy:  0.5882567  | \n",
      "Epoch  38 : \n",
      "Training loss:  1.16674042184641  |  Validation loss:  1.6069585407415365  | \n",
      " Training accuracy:  0.58714926  |  Validation accuracy:  0.5871493  | \n",
      "Epoch  39 : \n",
      "Training loss:  1.1664968983227886  |  Validation loss:  1.6060102328258048  | \n",
      " Training accuracy:  0.5849335  |  Validation accuracy:  0.58493364  | \n",
      "Epoch  40 : \n",
      "Training loss:  1.1663003295318624  |  Validation loss:  1.6052155843564428  | \n",
      " Training accuracy:  0.5834564  |  Validation accuracy:  0.58345664  | \n",
      "Epoch  41 : \n",
      "Training loss:  1.1661426193811644  |  Validation loss:  1.6047345885531343  | \n",
      " Training accuracy:  0.5845642  |  Validation accuracy:  0.58456427  | \n",
      "Epoch  42 : \n",
      "Training loss:  1.1660175879410202  |  Validation loss:  1.6043799023371628  | \n",
      " Training accuracy:  0.5834565  |  Validation accuracy:  0.5834566  | \n",
      "Epoch  43 : \n",
      "Training loss:  1.1659183191149423  |  Validation loss:  1.6042245707603635  | \n",
      " Training accuracy:  0.58124083  |  Validation accuracy:  0.5812408  | \n",
      "Epoch  44 : \n",
      "Training loss:  1.1658365416913963  |  Validation loss:  1.6041706919740468  | \n",
      " Training accuracy:  0.5790251  |  Validation accuracy:  0.579025  | \n",
      "Epoch  45 : \n",
      "Training loss:  1.1657711446166796  |  Validation loss:  1.6042515648331102  | \n",
      " Training accuracy:  0.5775481  |  Validation accuracy:  0.5775478  | \n",
      "Epoch  46 : \n",
      "Training loss:  1.1657166862981296  |  Validation loss:  1.6044019283569608  | \n",
      " Training accuracy:  0.5760709  |  Validation accuracy:  0.57607085  | \n",
      "Epoch  47 : \n",
      "Training loss:  1.1656718305178102  |  Validation loss:  1.6046190784822991  | \n",
      " Training accuracy:  0.5745938  |  Validation accuracy:  0.5745937  | \n",
      "Epoch  48 : \n",
      "Training loss:  1.1656342146370606  |  Validation loss:  1.6051330501371959  | \n",
      " Training accuracy:  0.5731168  |  Validation accuracy:  0.57311654  | \n",
      "Epoch  49 : \n",
      "Training loss:  1.1656037609211378  |  Validation loss:  1.6056607737970534  | \n",
      " Training accuracy:  0.570901  |  Validation accuracy:  0.57090133  | \n",
      "Epoch  50 : \n",
      "Training loss:  1.1655787484569546  |  Validation loss:  1.606157009237903  | \n",
      " Training accuracy:  0.56905466  |  Validation accuracy:  0.569055  | \n",
      "Epoch  51 : \n",
      "Training loss:  1.1655575368319537  |  Validation loss:  1.6066253142822555  | \n",
      " Training accuracy:  0.5683161  |  Validation accuracy:  0.5683164  | \n",
      "Epoch  52 : \n",
      "Training loss:  1.165539742258419  |  Validation loss:  1.6071972962015957  | \n",
      " Training accuracy:  0.5686854  |  Validation accuracy:  0.56868565  | \n",
      "Epoch  53 : \n",
      "Training loss:  1.1655245876470484  |  Validation loss:  1.607770228780058  | \n",
      " Training accuracy:  0.5686854  |  Validation accuracy:  0.5686857  | \n",
      "Epoch  54 : \n",
      "Training loss:  1.1655116714542175  |  Validation loss:  1.608398417599819  | \n",
      " Training accuracy:  0.567947  |  Validation accuracy:  0.56794715  | \n",
      "Epoch  55 : \n",
      "Training loss:  1.1655006065585742  |  Validation loss:  1.609295543320508  | \n",
      " Training accuracy:  0.5657311  |  Validation accuracy:  0.56573147  | \n",
      "Epoch  56 : \n",
      "Training loss:  1.1654912806578177  |  Validation loss:  1.6101875678636048  | \n",
      " Training accuracy:  0.5646234  |  Validation accuracy:  0.5646235  | \n",
      "Epoch  57 : \n",
      "Training loss:  1.1654830550907496  |  Validation loss:  1.611122935899295  | \n",
      " Training accuracy:  0.5635156  |  Validation accuracy:  0.5635155  | \n",
      "Epoch  58 : \n",
      "Training loss:  1.1654759310414076  |  Validation loss:  1.611885835051642  | \n",
      " Training accuracy:  0.5613  |  Validation accuracy:  0.5612996  | \n",
      "Epoch  59 : \n",
      "Training loss:  1.1654696341867261  |  Validation loss:  1.6126363862210766  | \n",
      " Training accuracy:  0.56019205  |  Validation accuracy:  0.5601918  | \n",
      "Epoch  60 : \n",
      "Training loss:  1.165464214658405  |  Validation loss:  1.6133872142617114  | \n",
      " Training accuracy:  0.5594535  |  Validation accuracy:  0.5594532  | \n",
      "Epoch  61 : \n",
      "Training loss:  1.165459501734479  |  Validation loss:  1.6140855299288637  | \n",
      " Training accuracy:  0.55945355  |  Validation accuracy:  0.5594532  | \n",
      "Epoch  62 : \n",
      "Training loss:  1.1654554813402198  |  Validation loss:  1.6152300863276932  | \n",
      " Training accuracy:  0.5572378  |  Validation accuracy:  0.5572377  | \n",
      "Epoch  63 : \n",
      "Training loss:  1.16545203890734  |  Validation loss:  1.6163743447208345  | \n",
      " Training accuracy:  0.55649924  |  Validation accuracy:  0.5564993  | \n",
      "Epoch  64 : \n",
      "Training loss:  1.165449139705203  |  Validation loss:  1.6175843004016033  | \n",
      " Training accuracy:  0.5564993  |  Validation accuracy:  0.5564994  | \n",
      "Epoch  65 : \n",
      "Training loss:  1.165446534263907  |  Validation loss:  1.6188233162502499  | \n",
      " Training accuracy:  0.55613005  |  Validation accuracy:  0.55613023  | \n",
      "Epoch  66 : \n",
      "Training loss:  1.1654443327813828  |  Validation loss:  1.61996811475568  | \n",
      " Training accuracy:  0.5550222  |  Validation accuracy:  0.5550224  | \n",
      "Epoch  67 : \n",
      "Training loss:  1.1654424053159471  |  Validation loss:  1.6210901298386389  | \n",
      " Training accuracy:  0.55502224  |  Validation accuracy:  0.5550223  | \n",
      "Epoch  68 : \n",
      "Training loss:  1.165440767863488  |  Validation loss:  1.6221392678470665  | \n",
      " Training accuracy:  0.5542837  |  Validation accuracy:  0.5542836  | \n",
      "Epoch  69 : \n",
      "Training loss:  1.1654394157655914  |  Validation loss:  1.6228719492970152  | \n",
      " Training accuracy:  0.55280656  |  Validation accuracy:  0.5528066  | \n",
      "Epoch  70 : \n",
      "Training loss:  1.16543824162591  |  Validation loss:  1.623533060814276  | \n",
      " Training accuracy:  0.552068  |  Validation accuracy:  0.5520682  | \n",
      "Epoch  71 : \n",
      "Training loss:  1.1654372194519262  |  Validation loss:  1.6241426354552364  | \n",
      " Training accuracy:  0.55206794  |  Validation accuracy:  0.55206805  | \n",
      "Epoch  72 : \n",
      "Training loss:  1.1654362722443545  |  Validation loss:  1.6247227984437096  | \n",
      " Training accuracy:  0.55280656  |  Validation accuracy:  0.5528067  | \n",
      "Epoch  73 : \n",
      "Training loss:  1.1654354168873213  |  Validation loss:  1.625373844278203  | \n",
      " Training accuracy:  0.552068  |  Validation accuracy:  0.5520682  | \n",
      "Epoch  74 : \n",
      "Training loss:  1.165434619415765  |  Validation loss:  1.6260170723677934  | \n",
      " Training accuracy:  0.55206794  |  Validation accuracy:  0.5520682  | \n",
      "Epoch  75 : \n",
      "Training loss:  1.1654339007225003  |  Validation loss:  1.626707048864249  | \n",
      " Training accuracy:  0.55132943  |  Validation accuracy:  0.5513297  | \n",
      "Epoch  76 : \n",
      "Training loss:  1.1654331996854481  |  Validation loss:  1.6274333420398972  | \n",
      " Training accuracy:  0.549483  |  Validation accuracy:  0.5494832  | \n",
      "Epoch  77 : \n",
      "Training loss:  1.1654325509672507  |  Validation loss:  1.6280915058241925  | \n",
      " Training accuracy:  0.55022156  |  Validation accuracy:  0.55022174  | \n",
      "Epoch  78 : \n",
      "Training loss:  1.1654319782416165  |  Validation loss:  1.6284304674537937  | \n",
      " Training accuracy:  0.55022156  |  Validation accuracy:  0.55022174  | \n",
      "Epoch  79 : \n",
      "Training loss:  1.165431439708806  |  Validation loss:  1.6287738263313984  | \n",
      " Training accuracy:  0.5498523  |  Validation accuracy:  0.54985243  | \n",
      "Epoch  80 : \n",
      "Training loss:  1.1654306197638313  |  Validation loss:  1.6298810743354688  | \n",
      " Training accuracy:  0.5491138  |  Validation accuracy:  0.5491137  | \n",
      "Epoch  81 : \n",
      "Training loss:  1.16542989568537  |  Validation loss:  1.6317013235112003  | \n",
      " Training accuracy:  0.5491138  |  Validation accuracy:  0.5491137  | \n",
      "Epoch  82 : \n",
      "Training loss:  1.1654292345892854  |  Validation loss:  1.6342695028391783  | \n",
      " Training accuracy:  0.5472674  |  Validation accuracy:  0.54726696  | \n",
      "Epoch  83 : \n",
      "Training loss:  1.1654286893639136  |  Validation loss:  1.636994257261969  | \n",
      " Training accuracy:  0.5450517  |  Validation accuracy:  0.5450514  | \n",
      "Epoch  84 : \n",
      "Training loss:  1.1654282441055437  |  Validation loss:  1.6396824994373924  | \n",
      " Training accuracy:  0.543944  |  Validation accuracy:  0.5439436  | \n",
      "Epoch  85 : \n",
      "Training loss:  1.1654278685318513  |  Validation loss:  1.642327924454205  | \n",
      " Training accuracy:  0.5424668  |  Validation accuracy:  0.5424666  | \n",
      "Epoch  86 : \n",
      "Training loss:  1.1654275524362196  |  Validation loss:  1.6449520064031706  | \n",
      " Training accuracy:  0.54135895  |  Validation accuracy:  0.5413591  | \n",
      "Epoch  87 : \n",
      "Training loss:  1.1654272861791548  |  Validation loss:  1.6475159831400585  | \n",
      " Training accuracy:  0.5409896  |  Validation accuracy:  0.5409898  | \n",
      "Epoch  88 : \n",
      "Training loss:  1.16542704455843  |  Validation loss:  1.6498180670667895  | \n",
      " Training accuracy:  0.5420976  |  Validation accuracy:  0.5420972  | \n",
      "Epoch  89 : \n",
      "Training loss:  1.165426826249272  |  Validation loss:  1.6510172599637767  | \n",
      " Training accuracy:  0.5424669  |  Validation accuracy:  0.5424664  | \n",
      "Epoch  90 : \n",
      "Training loss:  1.165426628217221  |  Validation loss:  1.6521074865518994  | \n",
      " Training accuracy:  0.5424669  |  Validation accuracy:  0.54246646  | \n",
      "Epoch  91 : \n",
      "Training loss:  1.165426450515182  |  Validation loss:  1.6531195860098085  | \n",
      " Training accuracy:  0.5424669  |  Validation accuracy:  0.54246646  | \n",
      "Epoch  92 : \n",
      "Training loss:  1.1654262823821395  |  Validation loss:  1.6540764459762403  | \n",
      " Training accuracy:  0.54062045  |  Validation accuracy:  0.54062074  | \n",
      "Epoch  93 : \n",
      "Training loss:  1.1654261321214119  |  Validation loss:  1.6549416893028697  | \n",
      " Training accuracy:  0.53951263  |  Validation accuracy:  0.53951275  | \n",
      "Epoch  94 : \n",
      "Training loss:  1.165425998572049  |  Validation loss:  1.6557297045843338  | \n",
      " Training accuracy:  0.53803545  |  Validation accuracy:  0.5380356  | \n",
      "Epoch  95 : \n",
      "Training loss:  1.165425874134738  |  Validation loss:  1.655509602287416  | \n",
      " Training accuracy:  0.5358199  |  Validation accuracy:  0.5358199  | \n",
      "Epoch  96 : \n",
      "Training loss:  1.165425766028131  |  Validation loss:  1.6554748270881081  | \n",
      " Training accuracy:  0.53655833  |  Validation accuracy:  0.5365585  | \n",
      "Epoch  97 : \n",
      "Training loss:  1.1654256595023509  |  Validation loss:  1.654973176431955  | \n",
      " Training accuracy:  0.5380354  |  Validation accuracy:  0.5380356  | \n",
      "Epoch  98 : \n",
      "Training loss:  1.1654255677908516  |  Validation loss:  1.654545485535266  | \n",
      " Training accuracy:  0.5391433  |  Validation accuracy:  0.5391434  | \n",
      "Epoch  99 : \n",
      "Training loss:  1.1654255122830932  |  Validation loss:  1.6534483227527184  | \n",
      " Training accuracy:  0.53692764  |  Validation accuracy:  0.5369279  | \n",
      "Epoch  100 : \n",
      "Training loss:  1.1654254565721214  |  Validation loss:  1.652390808995176  | \n",
      " Training accuracy:  0.5387741  |  Validation accuracy:  0.5387742  | \n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(n_layers = 2, eta = 0.01)\n",
    "\n",
    "MLP.fit(features, labels, 100, optimizer, \n",
    "           train_mask, val_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33926f5",
   "metadata": {},
   "source": [
    "Now let's test it as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebc3f0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training loss:  1.1654254565721214  |  Training accuracy:  0.5387741 \n",
      " Validation loss:  1.652390808995176  |  Validation accuracy:  0.5387742 \n",
      " Test loss:  1.6609423662965361  |  Test accuracy:  0.5387737\n"
     ]
    }
   ],
   "source": [
    "loss_train, accuracy_train = MLP.evaluate(features, labels, train_mask)\n",
    "loss_val, accuracy_val = MLP.evaluate(features, labels, val_mask)\n",
    "loss_test, accuracy_test = MLP.evaluate(features, labels, test_mask)\n",
    "\n",
    "print(' Training loss: ', loss_train, ' | ', 'Training accuracy: ', accuracy_train, '\\n', \n",
    "      'Validation loss: ', loss_val, ' | ', 'Validation accuracy: ', accuracy_val, '\\n',\n",
    "      'Test loss: ', loss_test, ' | ', 'Test accuracy: ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20404137",
   "metadata": {},
   "source": [
    "Quite a big drop in performance for all node sets. We can conclude that the structural information encoded in the edges (and thus the adjacency matrix) was crucial for the success of our GCN. The MLP, on the other hand, is completely oblivious of how the nodes it is being trained on are connected with each other, and with less information comes worse accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed655c0",
   "metadata": {},
   "source": [
    "### Scaling up the model\n",
    "A bigger model can usually capture more complex relationships amongst features and between features and labels. However, they increase the chances of overfitting (which is a real risk with a small trining dataset like ours). Let's test this by scaling up the model in two ways:\n",
    "\n",
    "-Adding units: we'll call this instance `GCN_wide`\n",
    "\n",
    "-Adding a layer: we'll call this instance `GCN_deep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec776f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 : \n",
      "Training loss:  1.9403334540476433  |  Validation loss:  1.9435950244965312  | \n",
      " Training accuracy:  0.74285716  |  Validation accuracy:  0.36599997  | \n",
      "Epoch  2 : \n",
      "Training loss:  1.9279835219417687  |  Validation loss:  1.9370433325381418  | \n",
      " Training accuracy:  0.92142856  |  Validation accuracy:  0.59199995  | \n",
      "Epoch  3 : \n",
      "Training loss:  1.9046159308855608  |  Validation loss:  1.9246803561936017  | \n",
      " Training accuracy:  0.9571429  |  Validation accuracy:  0.686  | \n",
      "Epoch  4 : \n",
      "Training loss:  1.8668441461575238  |  Validation loss:  1.9053950171398848  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.71999997  | \n",
      "Epoch  5 : \n",
      "Training loss:  1.8056552766042377  |  Validation loss:  1.8763497849978366  | \n",
      " Training accuracy:  0.97857153  |  Validation accuracy:  0.758  | \n",
      "Epoch  6 : \n",
      "Training loss:  1.719511615011205  |  Validation loss:  1.8363795501501317  | \n",
      " Training accuracy:  0.97857153  |  Validation accuracy:  0.77  | \n",
      "Epoch  7 : \n",
      "Training loss:  1.6117207367563076  |  Validation loss:  1.7827239651379418  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.7819999  | \n",
      "Epoch  8 : \n",
      "Training loss:  1.4987849778829745  |  Validation loss:  1.7201995627043878  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.79199994  | \n",
      "Epoch  9 : \n",
      "Training loss:  1.39950055889353  |  Validation loss:  1.6527723428549967  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.79399997  | \n",
      "Epoch  10 : \n",
      "Training loss:  1.322737590110443  |  Validation loss:  1.5903332224999893  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.79199994  | \n",
      "Epoch  11 : \n",
      "Training loss:  1.2670583452253517  |  Validation loss:  1.5392579721764246  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.784  | \n",
      "Epoch  12 : \n",
      "Training loss:  1.2308500980121977  |  Validation loss:  1.501163792771119  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.798  | \n",
      "Epoch  13 : \n",
      "Training loss:  1.2084263765507528  |  Validation loss:  1.4720874693559272  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.7819999  | \n",
      "Epoch  14 : \n",
      "Training loss:  1.194972332025532  |  Validation loss:  1.4471654170923507  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.7859999  | \n",
      "Epoch  15 : \n",
      "Training loss:  1.186191068161929  |  Validation loss:  1.4274957287167798  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.7879999  | \n",
      "Epoch  16 : \n",
      "Training loss:  1.180645309361445  |  Validation loss:  1.4136650387648109  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.78999996  | \n",
      "Epoch  17 : \n",
      "Training loss:  1.1774785958068525  |  Validation loss:  1.4029298074142378  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.7879999  | \n",
      "Epoch  18 : \n",
      "Training loss:  1.1756655129076008  |  Validation loss:  1.3972110329215588  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.78999996  | \n",
      "Epoch  19 : \n",
      "Training loss:  1.1734707869270542  |  Validation loss:  1.392961925457147  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.79199994  | \n",
      "Epoch  20 : \n",
      "Training loss:  1.1717549630948139  |  Validation loss:  1.392310740019322  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.7879999  | \n",
      "Epoch  21 : \n",
      "Training loss:  1.1691491315615392  |  Validation loss:  1.3911080182296482  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.7819999  | \n",
      "Epoch  22 : \n",
      "Training loss:  1.1664753803904304  |  Validation loss:  1.3901341351207468  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.7779999  | \n",
      "Epoch  23 : \n",
      "Training loss:  1.1658282750619364  |  Validation loss:  1.3902866855517981  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.7719999  | \n",
      "Epoch  24 : \n",
      "Training loss:  1.1657284103590415  |  Validation loss:  1.3902963560452168  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77  | \n",
      "Epoch  25 : \n",
      "Training loss:  1.1656883126602566  |  Validation loss:  1.3907586314586455  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77199996  | \n",
      "Epoch  26 : \n",
      "Training loss:  1.1656606067617186  |  Validation loss:  1.3912392353414178  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77  | \n",
      "Epoch  27 : \n",
      "Training loss:  1.16558838367311  |  Validation loss:  1.391125694469533  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76799995  | \n",
      "Epoch  28 : \n",
      "Training loss:  1.1655167076837956  |  Validation loss:  1.3918353982440461  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76399994  | \n",
      "Epoch  29 : \n",
      "Training loss:  1.1654805658792649  |  Validation loss:  1.3922856414260472  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.766  | \n",
      "Epoch  30 : \n",
      "Training loss:  1.1654632631639001  |  Validation loss:  1.3921201758984763  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77  | \n",
      "Epoch  31 : \n",
      "Training loss:  1.1654551067107697  |  Validation loss:  1.3925019787591328  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.766  | \n",
      "Epoch  32 : \n",
      "Training loss:  1.1654486517945444  |  Validation loss:  1.3935846617657057  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76799995  | \n",
      "Epoch  33 : \n",
      "Training loss:  1.1654470119855334  |  Validation loss:  1.3957245201855737  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.762  | \n",
      "Epoch  34 : \n",
      "Training loss:  1.16544712745673  |  Validation loss:  1.3976936599248144  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76400006  | \n",
      "Epoch  35 : \n",
      "Training loss:  1.165453618820951  |  Validation loss:  1.3996095284166796  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76600003  | \n",
      "Epoch  36 : \n",
      "Training loss:  1.1654466527292577  |  Validation loss:  1.4008813599390395  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76400006  | \n",
      "Epoch  37 : \n",
      "Training loss:  1.165437151653735  |  Validation loss:  1.4012104541572916  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  38 : \n",
      "Training loss:  1.1654324881213183  |  Validation loss:  1.4015882618462034  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  39 : \n",
      "Training loss:  1.165430086880619  |  Validation loss:  1.4020244541905464  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75600004  | \n",
      "Epoch  40 : \n",
      "Training loss:  1.1654291904225162  |  Validation loss:  1.4022117782417896  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75600004  | \n",
      "Epoch  41 : \n",
      "Training loss:  1.1654292104394544  |  Validation loss:  1.4016718369941508  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  42 : \n",
      "Training loss:  1.1654309668587322  |  Validation loss:  1.4010388510356713  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.762  | \n",
      "Epoch  43 : \n",
      "Training loss:  1.1654330969370168  |  Validation loss:  1.401083281314042  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  44 : \n",
      "Training loss:  1.1654384553266781  |  Validation loss:  1.4005857087925286  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  45 : \n",
      "Training loss:  1.1654491737015609  |  Validation loss:  1.4000348011845019  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  46 : \n",
      "Training loss:  1.1654355708916933  |  Validation loss:  1.401808694243964  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  47 : \n",
      "Training loss:  1.1654292025801956  |  Validation loss:  1.403382512291778  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  48 : \n",
      "Training loss:  1.1654256889098582  |  Validation loss:  1.4049470231749308  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.754  | \n",
      "Epoch  49 : \n",
      "Training loss:  1.1654239933500448  |  Validation loss:  1.406393560177676  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  50 : \n",
      "Training loss:  1.1654232257479444  |  Validation loss:  1.4076847204529  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  51 : \n",
      "Training loss:  1.165422936093927  |  Validation loss:  1.4088183632244113  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  52 : \n",
      "Training loss:  1.1654229368762885  |  Validation loss:  1.409957947667746  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  53 : \n",
      "Training loss:  1.1654232339567736  |  Validation loss:  1.4108893092503274  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  54 : \n",
      "Training loss:  1.1654239090450529  |  Validation loss:  1.4116748216849178  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  55 : \n",
      "Training loss:  1.1654250989431263  |  Validation loss:  1.4123049589013612  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  56 : \n",
      "Training loss:  1.1654264318825565  |  Validation loss:  1.4127461176364509  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75200003  | \n",
      "Epoch  57 : \n",
      "Training loss:  1.1654285139019005  |  Validation loss:  1.413147884678765  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  58 : \n",
      "Training loss:  1.1654310926907305  |  Validation loss:  1.4134924320310955  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  59 : \n",
      "Training loss:  1.1654334921915888  |  Validation loss:  1.4135949457725911  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  60 : \n",
      "Training loss:  1.1654361942792466  |  Validation loss:  1.4136869581280658  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  61 : \n",
      "Training loss:  1.1654386325984891  |  Validation loss:  1.4137593046213859  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  62 : \n",
      "Training loss:  1.1654393339505342  |  Validation loss:  1.4137959315309896  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  63 : \n",
      "Training loss:  1.1654400737612998  |  Validation loss:  1.4138558085700677  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  64 : \n",
      "Training loss:  1.1654391289435702  |  Validation loss:  1.413877612315649  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  65 : \n",
      "Training loss:  1.1654296558017019  |  Validation loss:  1.413660671605321  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  66 : \n",
      "Training loss:  1.1654257297491923  |  Validation loss:  1.4134772711177699  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  67 : \n",
      "Training loss:  1.165423999504789  |  Validation loss:  1.4132131998924145  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.746  | \n",
      "Epoch  68 : \n",
      "Training loss:  1.1654232049480073  |  Validation loss:  1.4129850884624189  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.746  | \n",
      "Epoch  69 : \n",
      "Training loss:  1.1654227711952736  |  Validation loss:  1.4127445765819293  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.746  | \n",
      "Epoch  70 : \n",
      "Training loss:  1.1654225590029548  |  Validation loss:  1.4125081126278705  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74399996  | \n",
      "Epoch  71 : \n",
      "Training loss:  1.1654224467937755  |  Validation loss:  1.4121262795054934  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.746  | \n",
      "Epoch  72 : \n",
      "Training loss:  1.1654223939051882  |  Validation loss:  1.4117470061237998  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.746  | \n",
      "Epoch  73 : \n",
      "Training loss:  1.1654223745520234  |  Validation loss:  1.4113633696823602  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  74 : \n",
      "Training loss:  1.165422374393408  |  Validation loss:  1.4110044936492852  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  75 : \n",
      "Training loss:  1.1654223878844192  |  Validation loss:  1.4101795712155698  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.746  | \n",
      "Epoch  76 : \n",
      "Training loss:  1.1654224161770175  |  Validation loss:  1.4093341368314265  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74799997  | \n",
      "Epoch  77 : \n",
      "Training loss:  1.1654224541650742  |  Validation loss:  1.4084964554013213  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  78 : \n",
      "Training loss:  1.1654224981718668  |  Validation loss:  1.407612235798623  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75200003  | \n",
      "Epoch  79 : \n",
      "Training loss:  1.1654225479341354  |  Validation loss:  1.4067729479534483  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.754  | \n",
      "Epoch  80 : \n",
      "Training loss:  1.165422601992783  |  Validation loss:  1.4059904520555873  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75600004  | \n",
      "Epoch  81 : \n",
      "Training loss:  1.1654226653036848  |  Validation loss:  1.4052506659501443  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75600004  | \n",
      "Epoch  82 : \n",
      "Training loss:  1.165422734962663  |  Validation loss:  1.404560849935667  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75600004  | \n",
      "Epoch  83 : \n",
      "Training loss:  1.1654227822579206  |  Validation loss:  1.4039506919174585  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75600004  | \n",
      "Epoch  84 : \n",
      "Training loss:  1.165422822970146  |  Validation loss:  1.4033966809075376  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  85 : \n",
      "Training loss:  1.1654225117382584  |  Validation loss:  1.4037031007736058  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75600004  | \n",
      "Epoch  86 : \n",
      "Training loss:  1.1654223835412274  |  Validation loss:  1.4038384165205544  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  87 : \n",
      "Training loss:  1.1654223316528676  |  Validation loss:  1.403836067534695  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75600004  | \n",
      "Epoch  88 : \n",
      "Training loss:  1.165422304326424  |  Validation loss:  1.403908572466241  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75600004  | \n",
      "Epoch  89 : \n",
      "Training loss:  1.1654222912845111  |  Validation loss:  1.403945027812425  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  90 : \n",
      "Training loss:  1.1654222848205424  |  Validation loss:  1.4039396794927494  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  91 : \n",
      "Training loss:  1.1654222815405284  |  Validation loss:  1.4038055908268754  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  92 : \n",
      "Training loss:  1.1654222766230204  |  Validation loss:  1.403296369266133  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  93 : \n",
      "Training loss:  1.1654222746125014  |  Validation loss:  1.402842672699025  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  94 : \n",
      "Training loss:  1.16542227492355  |  Validation loss:  1.4023811918056395  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  95 : \n",
      "Training loss:  1.1654222767557896  |  Validation loss:  1.4019226796116337  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  96 : \n",
      "Training loss:  1.1654222794751283  |  Validation loss:  1.4015073604370616  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  97 : \n",
      "Training loss:  1.1654222848139795  |  Validation loss:  1.4010761976982868  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76399994  | \n",
      "Epoch  98 : \n",
      "Training loss:  1.1654222918192916  |  Validation loss:  1.4006919597026717  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76399994  | \n",
      "Epoch  99 : \n",
      "Training loss:  1.16542229959426  |  Validation loss:  1.4003495655263094  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76399994  | \n",
      "Epoch  100 : \n",
      "Training loss:  1.1654223079428223  |  Validation loss:  1.4000471078002905  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76399994  | \n"
     ]
    }
   ],
   "source": [
    "GCN_wide = GNN(features.shape[1], adj_norm, [256, 7], 0.5)\n",
    "\n",
    "optimizer = Adam(n_layers = 2)\n",
    "\n",
    "GCN_wide.fit(features, labels, 100, optimizer, \n",
    "           train_mask, val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95f00c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training loss:  1.1654223079428223  |  Training accuracy:  1.0 \n",
      " Validation loss:  1.4000471078002905  |  Validation accuracy:  0.76399994 \n",
      " Test loss:  1.3774315441639304  |  Test accuracy:  0.78699994\n"
     ]
    }
   ],
   "source": [
    "loss_train, accuracy_train = GCN_wide.evaluate(features, labels, train_mask)\n",
    "loss_val, accuracy_val = GCN_wide.evaluate(features, labels, val_mask)\n",
    "loss_test, accuracy_test = GCN_wide.evaluate(features, labels, test_mask)\n",
    "\n",
    "print(' Training loss: ', loss_train, ' | ', 'Training accuracy: ', accuracy_train, '\\n', \n",
    "      'Validation loss: ', loss_val, ' | ', 'Validation accuracy: ', accuracy_val, '\\n',\n",
    "      'Test loss: ', loss_test, ' | ', 'Test accuracy: ', accuracy_test)\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182902a5",
   "metadata": {},
   "source": [
    "We observe that adding hidden units made very little difference. Let's try adding layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d0f78d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 : \n",
      "Training loss:  1.9457447059766688  |  Validation loss:  1.9463379127316653  | \n",
      " Training accuracy:  0.14285715  |  Validation accuracy:  0.058  | \n",
      "Epoch  2 : \n",
      "Training loss:  1.945044971173609  |  Validation loss:  1.94628358712265  | \n",
      " Training accuracy:  0.19999999  |  Validation accuracy:  0.094  | \n",
      "Epoch  3 : \n",
      "Training loss:  1.942894483201862  |  Validation loss:  1.9452578742518623  | \n",
      " Training accuracy:  0.40714285  |  Validation accuracy:  0.21799998  | \n",
      "Epoch  4 : \n",
      "Training loss:  1.9388657232655477  |  Validation loss:  1.9429934689187005  | \n",
      " Training accuracy:  0.39999998  |  Validation accuracy:  0.27199998  | \n",
      "Epoch  5 : \n",
      "Training loss:  1.9308096415435314  |  Validation loss:  1.9383401606754547  | \n",
      " Training accuracy:  0.4214286  |  Validation accuracy:  0.29  | \n",
      "Epoch  6 : \n",
      "Training loss:  1.9171176317332617  |  Validation loss:  1.9301181192502503  | \n",
      " Training accuracy:  0.5642857  |  Validation accuracy:  0.35599998  | \n",
      "Epoch  7 : \n",
      "Training loss:  1.8953187971750796  |  Validation loss:  1.9178710547365152  | \n",
      " Training accuracy:  0.6285714  |  Validation accuracy:  0.39  | \n",
      "Epoch  8 : \n",
      "Training loss:  1.8625162675857736  |  Validation loss:  1.900110657843221  | \n",
      " Training accuracy:  0.6642857  |  Validation accuracy:  0.38199997  | \n",
      "Epoch  9 : \n",
      "Training loss:  1.8151285194943487  |  Validation loss:  1.8741363950401384  | \n",
      " Training accuracy:  0.6571429  |  Validation accuracy:  0.35999998  | \n",
      "Epoch  10 : \n",
      "Training loss:  1.7608236196912317  |  Validation loss:  1.8449622158153665  | \n",
      " Training accuracy:  0.69285715  |  Validation accuracy:  0.35599998  | \n",
      "Epoch  11 : \n",
      "Training loss:  1.7062625528186735  |  Validation loss:  1.8148450068080824  | \n",
      " Training accuracy:  0.7071429  |  Validation accuracy:  0.364  | \n",
      "Epoch  12 : \n",
      "Training loss:  1.6525588371274231  |  Validation loss:  1.7875797443220531  | \n",
      " Training accuracy:  0.7285714  |  Validation accuracy:  0.38599998  | \n",
      "Epoch  13 : \n",
      "Training loss:  1.6024657114036387  |  Validation loss:  1.7669048504829683  | \n",
      " Training accuracy:  0.76428574  |  Validation accuracy:  0.39599997  | \n",
      "Epoch  14 : \n",
      "Training loss:  1.5484422402074667  |  Validation loss:  1.7384727734302965  | \n",
      " Training accuracy:  0.7714286  |  Validation accuracy:  0.434  | \n",
      "Epoch  15 : \n",
      "Training loss:  1.4984027941347464  |  Validation loss:  1.7151323695153793  | \n",
      " Training accuracy:  0.8000001  |  Validation accuracy:  0.466  | \n",
      "Epoch  16 : \n",
      "Training loss:  1.4659576236022138  |  Validation loss:  1.7214551549085906  | \n",
      " Training accuracy:  0.7857143  |  Validation accuracy:  0.47799993  | \n",
      "Epoch  17 : \n",
      "Training loss:  1.4102204664717584  |  Validation loss:  1.6874096277856148  | \n",
      " Training accuracy:  0.8428572  |  Validation accuracy:  0.53  | \n",
      "Epoch  18 : \n",
      "Training loss:  1.3369263510785456  |  Validation loss:  1.609061359028182  | \n",
      " Training accuracy:  0.9142857  |  Validation accuracy:  0.626  | \n",
      "Epoch  19 : \n",
      "Training loss:  1.3013947732753186  |  Validation loss:  1.5521816992201471  | \n",
      " Training accuracy:  0.92857146  |  Validation accuracy:  0.69399995  | \n",
      "Epoch  20 : \n",
      "Training loss:  1.2936144044705808  |  Validation loss:  1.5310298118676149  | \n",
      " Training accuracy:  0.9357143  |  Validation accuracy:  0.68999994  | \n",
      "Epoch  21 : \n",
      "Training loss:  1.2523154533783156  |  Validation loss:  1.4744868002800702  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.738  | \n",
      "Epoch  22 : \n",
      "Training loss:  1.2282574224062346  |  Validation loss:  1.4341860219144507  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.77199996  | \n",
      "Epoch  23 : \n",
      "Training loss:  1.2187053914911734  |  Validation loss:  1.4189422038288946  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.782  | \n",
      "Epoch  24 : \n",
      "Training loss:  1.2086224882999514  |  Validation loss:  1.4076356390043394  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.7859999  | \n",
      "Epoch  25 : \n",
      "Training loss:  1.2012631307032684  |  Validation loss:  1.4142065873210077  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.76999986  | \n",
      "Epoch  26 : \n",
      "Training loss:  1.1936624833980918  |  Validation loss:  1.423932020594042  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.74999994  | \n",
      "Epoch  27 : \n",
      "Training loss:  1.1893285299427276  |  Validation loss:  1.4294260142752175  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.73800004  | \n",
      "Epoch  28 : \n",
      "Training loss:  1.1850404514707709  |  Validation loss:  1.4256648612404723  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.738  | \n",
      "Epoch  29 : \n",
      "Training loss:  1.1859585125813692  |  Validation loss:  1.4214717314887693  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.74399996  | \n",
      "Epoch  30 : \n",
      "Training loss:  1.1948199466279477  |  Validation loss:  1.4334117795171915  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.73199993  | \n",
      "Epoch  31 : \n",
      "Training loss:  1.2029495101213705  |  Validation loss:  1.435566598762202  | \n",
      " Training accuracy:  0.9642858  |  Validation accuracy:  0.728  | \n",
      "Epoch  32 : \n",
      "Training loss:  1.2109651883873984  |  Validation loss:  1.4388267178516958  | \n",
      " Training accuracy:  0.9642858  |  Validation accuracy:  0.716  | \n",
      "Epoch  33 : \n",
      "Training loss:  1.1994500724178623  |  Validation loss:  1.4388929254549245  | \n",
      " Training accuracy:  0.97142863  |  Validation accuracy:  0.72  | \n",
      "Epoch  34 : \n",
      "Training loss:  1.198178552292655  |  Validation loss:  1.451832666841596  | \n",
      " Training accuracy:  0.9642857  |  Validation accuracy:  0.70799994  | \n",
      "Epoch  35 : \n",
      "Training loss:  1.1864478093768174  |  Validation loss:  1.4423084128306696  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.722  | \n",
      "Epoch  36 : \n",
      "Training loss:  1.1806595610358184  |  Validation loss:  1.4330437415156307  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.726  | \n",
      "Epoch  37 : \n",
      "Training loss:  1.1807768105255625  |  Validation loss:  1.4289500287665606  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.736  | \n",
      "Epoch  38 : \n",
      "Training loss:  1.1787841726871997  |  Validation loss:  1.4286652820594266  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.73999995  | \n",
      "Epoch  39 : \n",
      "Training loss:  1.1792143457169049  |  Validation loss:  1.435299326996924  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.728  | \n",
      "Epoch  40 : \n",
      "Training loss:  1.1784642208169942  |  Validation loss:  1.438604085744202  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.734  | \n",
      "Epoch  41 : \n",
      "Training loss:  1.1767413427859927  |  Validation loss:  1.4378283227375026  | \n",
      " Training accuracy:  0.9857144  |  Validation accuracy:  0.736  | \n",
      "Epoch  42 : \n",
      "Training loss:  1.1743035046752974  |  Validation loss:  1.4301427353130454  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.738  | \n",
      "Epoch  43 : \n",
      "Training loss:  1.171813422239726  |  Validation loss:  1.4211672443386392  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.73999995  | \n",
      "Epoch  44 : \n",
      "Training loss:  1.1693765384045267  |  Validation loss:  1.4150674574521296  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74999994  | \n",
      "Epoch  45 : \n",
      "Training loss:  1.1677786946567132  |  Validation loss:  1.4067886631947712  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  46 : \n",
      "Training loss:  1.1698911703155606  |  Validation loss:  1.3960136571662611  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.77  | \n",
      "Epoch  47 : \n",
      "Training loss:  1.1701345497820945  |  Validation loss:  1.3933109566347703  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.77599996  | \n",
      "Epoch  48 : \n",
      "Training loss:  1.1695289340440684  |  Validation loss:  1.3918591392652782  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.778  | \n",
      "Epoch  49 : \n",
      "Training loss:  1.1710320611261893  |  Validation loss:  1.3867730938128857  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.782  | \n",
      "Epoch  50 : \n",
      "Training loss:  1.171077871385162  |  Validation loss:  1.3791174703931584  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.7879999  | \n",
      "Epoch  51 : \n",
      "Training loss:  1.1707065447360117  |  Validation loss:  1.3782186240256433  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.78999996  | \n",
      "Epoch  52 : \n",
      "Training loss:  1.1696591500052436  |  Validation loss:  1.3786448882971423  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.7839999  | \n",
      "Epoch  53 : \n",
      "Training loss:  1.1665125411349315  |  Validation loss:  1.37850022536335  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.7819999  | \n",
      "Epoch  54 : \n",
      "Training loss:  1.1656920829862045  |  Validation loss:  1.3778045342063412  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.78999996  | \n",
      "Epoch  55 : \n",
      "Training loss:  1.165550025747259  |  Validation loss:  1.3797350997902984  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.7859999  | \n",
      "Epoch  56 : \n",
      "Training loss:  1.16557962079888  |  Validation loss:  1.3846653427118334  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.782  | \n",
      "Epoch  57 : \n",
      "Training loss:  1.1658728139605532  |  Validation loss:  1.390504052352025  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76799995  | \n",
      "Epoch  58 : \n",
      "Training loss:  1.166588829168659  |  Validation loss:  1.3961199993716142  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76799995  | \n",
      "Epoch  59 : \n",
      "Training loss:  1.1670981129693045  |  Validation loss:  1.404971316829965  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.756  | \n",
      "Epoch  60 : \n",
      "Training loss:  1.1690995784436955  |  Validation loss:  1.4096485899703077  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.75999993  | \n",
      "Epoch  61 : \n",
      "Training loss:  1.1689280549861067  |  Validation loss:  1.4094052140725073  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75799996  | \n",
      "Epoch  62 : \n",
      "Training loss:  1.1682940717523544  |  Validation loss:  1.4068041391754822  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.766  | \n",
      "Epoch  63 : \n",
      "Training loss:  1.1674545304067903  |  Validation loss:  1.4090368696308926  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75200003  | \n",
      "Epoch  64 : \n",
      "Training loss:  1.16795074395957  |  Validation loss:  1.4122212151039206  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.752  | \n",
      "Epoch  65 : \n",
      "Training loss:  1.1662311359874287  |  Validation loss:  1.4156362100879887  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.746  | \n",
      "Epoch  66 : \n",
      "Training loss:  1.165705325369799  |  Validation loss:  1.4196779765594738  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.746  | \n",
      "Epoch  67 : \n",
      "Training loss:  1.1654311323082585  |  Validation loss:  1.4236574913809683  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74200004  | \n",
      "Epoch  68 : \n",
      "Training loss:  1.1654351455599559  |  Validation loss:  1.4257337408348798  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.73200005  | \n",
      "Epoch  69 : \n",
      "Training loss:  1.1655637887333206  |  Validation loss:  1.4221384375328114  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.742  | \n",
      "Epoch  70 : \n",
      "Training loss:  1.1658999987858518  |  Validation loss:  1.421961688812908  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74399996  | \n",
      "Epoch  71 : \n",
      "Training loss:  1.167043898204999  |  Validation loss:  1.424906938003596  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.734  | \n",
      "Epoch  72 : \n",
      "Training loss:  1.1658426292240778  |  Validation loss:  1.4202602759797187  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.74599993  | \n",
      "Epoch  73 : \n",
      "Training loss:  1.1655282848606046  |  Validation loss:  1.4152597628934533  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75  | \n",
      "Epoch  74 : \n",
      "Training loss:  1.1654541796255777  |  Validation loss:  1.4099729072570668  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.756  | \n",
      "Epoch  75 : \n",
      "Training loss:  1.165429808432349  |  Validation loss:  1.4077991465543342  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76000005  | \n",
      "Epoch  76 : \n",
      "Training loss:  1.165424071442756  |  Validation loss:  1.4080413755222676  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  77 : \n",
      "Training loss:  1.165423012836264  |  Validation loss:  1.4084383866121395  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.752  | \n",
      "Epoch  78 : \n",
      "Training loss:  1.1654226096893665  |  Validation loss:  1.408107040051389  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75200003  | \n",
      "Epoch  79 : \n",
      "Training loss:  1.1654227260688979  |  Validation loss:  1.4043413794415613  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.758  | \n",
      "Epoch  80 : \n",
      "Training loss:  1.1654235429248596  |  Validation loss:  1.4005986324745558  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.762  | \n",
      "Epoch  81 : \n",
      "Training loss:  1.1654238866181896  |  Validation loss:  1.3973504336782727  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76600003  | \n",
      "Epoch  82 : \n",
      "Training loss:  1.165425249360969  |  Validation loss:  1.3918859039770688  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77199996  | \n",
      "Epoch  83 : \n",
      "Training loss:  1.1654481911453676  |  Validation loss:  1.3912943133819051  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.774  | \n",
      "Epoch  84 : \n",
      "Training loss:  1.1655463374410076  |  Validation loss:  1.3926152678748882  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77199996  | \n",
      "Epoch  85 : \n",
      "Training loss:  1.1660233920574388  |  Validation loss:  1.3943619665094256  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77  | \n",
      "Epoch  86 : \n",
      "Training loss:  1.1674554933381565  |  Validation loss:  1.3961122573613218  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76800007  | \n",
      "Epoch  87 : \n",
      "Training loss:  1.1696411365520518  |  Validation loss:  1.3972086472521652  | \n",
      " Training accuracy:  0.99285716  |  Validation accuracy:  0.77000004  | \n",
      "Epoch  88 : \n",
      "Training loss:  1.168475598469733  |  Validation loss:  1.3983708611337207  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76400006  | \n",
      "Epoch  89 : \n",
      "Training loss:  1.1671114698037894  |  Validation loss:  1.3994527922292963  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76799995  | \n",
      "Epoch  90 : \n",
      "Training loss:  1.1657057742926018  |  Validation loss:  1.3965228557825253  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.766  | \n",
      "Epoch  91 : \n",
      "Training loss:  1.1654666506073212  |  Validation loss:  1.3941985771566061  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.76799995  | \n",
      "Epoch  92 : \n",
      "Training loss:  1.1654345329187452  |  Validation loss:  1.3931253544265298  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77599996  | \n",
      "Epoch  93 : \n",
      "Training loss:  1.1654257789028366  |  Validation loss:  1.3955465619556073  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.77  | \n",
      "Epoch  94 : \n",
      "Training loss:  1.1654248322749403  |  Validation loss:  1.4012569503569348  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.762  | \n",
      "Epoch  95 : \n",
      "Training loss:  1.1654277476273447  |  Validation loss:  1.4066457795458405  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.756  | \n",
      "Epoch  96 : \n",
      "Training loss:  1.1654283384365287  |  Validation loss:  1.4081441861452284  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.756  | \n",
      "Epoch  97 : \n",
      "Training loss:  1.1654255753759089  |  Validation loss:  1.409304374180661  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.756  | \n",
      "Epoch  98 : \n",
      "Training loss:  1.165424006975575  |  Validation loss:  1.410295322309082  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.756  | \n",
      "Epoch  99 : \n",
      "Training loss:  1.1654233590142318  |  Validation loss:  1.4119545346732707  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75399995  | \n",
      "Epoch  100 : \n",
      "Training loss:  1.1654231496776957  |  Validation loss:  1.4147507217815862  | \n",
      " Training accuracy:  1.0  |  Validation accuracy:  0.75399995  | \n"
     ]
    }
   ],
   "source": [
    "GCN_deep = GNN(features.shape[1], adj_norm, [256, 64, 7], 0.5)\n",
    "\n",
    "optimizer = Adam(n_layers = 3)\n",
    "\n",
    "GCN_deep.fit(features, labels, 100, optimizer, \n",
    "           train_mask, val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efd1f739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training loss:  1.1654231496776957  |  Training accuracy:  1.0 \n",
      " Validation loss:  1.4147507217815862  |  Validation accuracy:  0.75399995 \n",
      " Test loss:  1.3996099499461363  |  Test accuracy:  0.7649999\n"
     ]
    }
   ],
   "source": [
    "loss_train, accuracy_train = GCN_deep.evaluate(features, labels, train_mask)\n",
    "loss_val, accuracy_val = GCN_deep.evaluate(features, labels, val_mask)\n",
    "loss_test, accuracy_test = GCN_deep.evaluate(features, labels, test_mask)\n",
    "\n",
    "print(' Training loss: ', loss_train, ' | ', 'Training accuracy: ', accuracy_train, '\\n', \n",
    "      'Validation loss: ', loss_val, ' | ', 'Validation accuracy: ', accuracy_val, '\\n',\n",
    "      'Test loss: ', loss_test, ' | ', 'Test accuracy: ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98acdd9",
   "metadata": {},
   "source": [
    "Again, very little difference in terms of performance. The only things worth noting about `GCN_wide` and `GCN_deep` are the following:\n",
    "\n",
    "-The models take longer to train. Of course this is expected, as bigger matrices entail a bigger computational cost.\n",
    "\n",
    "-The models take longer to converge. Again, no surprise here, as the optimizer has to navigate a higher-dimensional space to minimise the loss.\n",
    "\n",
    "-There is no major increase in overfitting. This is somewhat surprising, but it is most likely due to the dropout layers. In fact, even when I tried the original `GCN` model without dropout, it would overfit much more, with validation and test accuracies never exceeding 55% (while the training accuracy got to 100% very quickly). I did not include this experiment for brevity. \n",
    "\n",
    "We can thus conclude that there is no real benefit in increasing the size of the model in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48062d3",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "In this notebook we implemented a GCN model from scratch, using numpy only. We further veryfied the importance of the graph structure encoded in the adjacency matrix by comparing our model with a simple MLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e6587",
   "metadata": {},
   "source": [
    "### References\n",
    "Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n",
    "\n",
    "Kipf, T. N., & Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.\n",
    "\n",
    "Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., & Eliassi-Rad, T. (2008). Collective classification in network data. AI magazine, 29(3), 93-93.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a77ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
